{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b0883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a numpy array with size nrows x ncolumns-1. nrows and ncolums are the rows and columns of the dataset\n",
    "#the Date column is skipped (ncolumns-1)\n",
    "def readData(fname):\n",
    "    with open(fname) as f:\n",
    "        fileData = f.read()\n",
    "  \n",
    "    lines = fileData.split(\"\\n\")\n",
    "    header = lines[0].split(\",\")\n",
    "    lines = lines[1:] \n",
    "    #print(header) \n",
    "    #print(\"Data rows: \", len(lines))\n",
    "\n",
    "    rawData = np.zeros((len(lines), len(header)-1)) #skip the Date column\n",
    "\n",
    "    for i, aLine in enumerate(lines):       \n",
    "        splittedLine = aLine.split(\",\")[:]\n",
    "        rawData[i, 0] = splittedLine[0]\n",
    "        rawData[i, 1:] = [float(x) for x in splittedLine[2:]] \n",
    "\n",
    "    return rawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cedb758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the train and test data, normalized. It also returns the standard deviation of Weekly_Sales\n",
    "#Each list has a size equal to the number of stores\n",
    "#For each store there is a list of size trainNSamples (testNSamples) x nColums-1 (the store id is skipped)\n",
    "#Columns: Weekly_Sales,Holiday_Flag,Temperature,Fuel_Price,CPI,Unemployment\n",
    "def splitTrainTest(rawData, testPercent):\n",
    "\n",
    "    listStore = np.unique(rawData[:, 0])\n",
    "    trainNSamples = np.zeros(len(listStore))\n",
    "    \n",
    "    for i, storeId in enumerate(listStore):\n",
    "        trainNSamples[i] = np.count_nonzero(rawData[:, 0] == storeId)\n",
    "    trainNSamples = np.floor((1-testPercent) *  trainNSamples)\n",
    "\n",
    "    tmpTrain = np.zeros((int(np.sum(trainNSamples)), len(rawData[0])))\n",
    "\n",
    "    store = -1\n",
    "    counter = 0\n",
    "    counterTrain = 0\n",
    "    storeDict = dict(zip(listStore, trainNSamples))\n",
    "    for i, aLine in enumerate(rawData):\n",
    "        if store != aLine[0]:\n",
    "            store = int(aLine[0])\n",
    "            counter = 0\n",
    "        if(counter < storeDict.get(store)):\n",
    "            tmpTrain[counterTrain] = rawData[i][:]\n",
    "            counterTrain += 1\n",
    "            counter += 1\n",
    "\n",
    "    meanData = tmpTrain.mean(axis=0)\n",
    "    stdData = tmpTrain.std(axis=0)\n",
    "    rawNormData = (rawData - meanData) / stdData\n",
    "\n",
    "    allTrain = list()\n",
    "    allTest = list()\n",
    "    store = -1\n",
    "    counter = 0\n",
    "    for i, aLine in enumerate(rawNormData):\n",
    "        splittedLine = [float(x) for x in aLine[1:]] #skip store id\n",
    "        if store != rawData[i][0]:\n",
    "            if i != 0:\n",
    "                allTrain.append(storeDataTrain)\n",
    "                allTest.append(storeDataTest)\n",
    "            store = int(rawData[i][0])\n",
    "            storeDataTrain = list()\n",
    "            storeDataTest = list()\n",
    "            counter = 0\n",
    "\n",
    "        if(counter < storeDict.get(store)):\n",
    "            storeDataTrain.append(splittedLine)\n",
    "            counter += 1\n",
    "        else:\n",
    "            storeDataTest.append(splittedLine)\n",
    "\n",
    "        if i == len(rawNormData)-1:\n",
    "            allTrain.append(storeDataTrain)\n",
    "            allTest.append(storeDataTest)\n",
    "\n",
    "    return allTrain, allTest, stdData[1] #std of wSales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates a time series given the input and ouput data, the sequence length and the batch size\n",
    "#seqLength is the number of weeks (observations) of data to be used as input\n",
    "#the target will be the weekly sales in 2 weeks\n",
    "def generateTimeSeries(data, wSales, seqLength, batchSize):   \n",
    "    sampling_rate = 1 #keep all the data points \n",
    "    weeksInAdvance = 3\n",
    "    delay = sampling_rate * (seqLength + weeksInAdvance - 1) #the target will be the weekly sales in 2 weeks\n",
    "    \n",
    "    dataset = keras.utils.timeseries_dataset_from_array(\n",
    "        data[:-delay],\n",
    "        targets=wSales[delay:],\n",
    "        sampling_rate=sampling_rate,\n",
    "        sequence_length=seqLength,\n",
    "        shuffle=True,\n",
    "        batch_size=batchSize,\n",
    "        start_index=0)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09807fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTimeSeriesList(theList):\n",
    "    print('list length', len(theList))\n",
    "    print('First element')\n",
    "    input, target = theList[0]\n",
    "    print([float(x) for x in input.numpy().flatten()], [float(x) for x in target.numpy().flatten()])\n",
    "    print('Last element')\n",
    "    input, target = theList[-1]\n",
    "    print([float(x) for x in input.numpy().flatten()], [float(x) for x in target.numpy().flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb5250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the training and test time series\n",
    "#it also returns the standard deviation of Weekly_Sales, and the number of input features\n",
    "def generateTrainTestData(fileName, testPercent, seqLength, batchSize):\n",
    "    rawData = readData(os.path.join(fileName))\n",
    "    allTrain, allTest, stdSales = splitTrainTest(rawData, testPercent)\n",
    "    \n",
    "    for i in range(len(allTrain)):\n",
    "        tmp_train = generateTimeSeries(np.array(allTrain[i]), np.array(allTrain[i])[:,0], seqLength, batchSize)\n",
    "        tmp_test = generateTimeSeries(np.array(allTest[i]), np.array(allTest[i])[:,0], seqLength, batchSize)\n",
    "\n",
    "        if i == 0:\n",
    "            train_dataset = tmp_train\n",
    "            test_dataset = tmp_test\n",
    "        else:\n",
    "            train_dataset = train_dataset.concatenate(tmp_train)\n",
    "            test_dataset = test_dataset.concatenate(tmp_test)\n",
    "    \n",
    "    return train_dataset, test_dataset, stdSales, np.shape(allTrain)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generateTrainTestData(fileName, testPercent, seqLength, batchSize):\n",
    "#trainData, testData: each element comes from keras.utils.timeseries_dataset_from_array, i.e., is a time series\n",
    "#Columns: Weekly_Sales,Holiday_Flag,Temperature,Fuel_Price,CPI,Unemployment\n",
    "\n",
    "testPercent = 0.2\n",
    "seqLength = 16\n",
    "batchSize = 1\n",
    "trainData, testData, stdSales, nFeatures = generateTrainTestData(\"data/walmart-sales-dataset-of-45stores.csv\",\n",
    "    testPercent, seqLength, batchSize) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "580e160b",
   "metadata": {},
   "source": [
    "# Approaches\n",
    "- Basic RNN\n",
    "- Basic GRU (best performing: 64203 MAE)\n",
    "- Recurrent dropout LSTM (worst performing: 109091 MAE)\n",
    "- Stacked GRU layers\n",
    "- Bidirectional LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78a69562",
   "metadata": {},
   "source": [
    "# Basic RNN\n",
    "\n",
    "This was the first tested approach (all discussion regarding the results takes into account 68.000 as a good result, as stipulated by the problem description), and its results aren't good at all. The dimensionalities tested were 16, 32 and 48, and neither of them showed good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(seqLength, 6))\n",
    "\n",
    "x = layers.SimpleRNN(48)(inputs)\n",
    "\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "  \n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models2/walmart_basic_rnn.keras\", save_best_only=True, monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "#print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd97527",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(trainData,\n",
    "                    epochs=50,\n",
    "                    validation_data=testData,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "#model = keras.models.load_model(\"walmart_basic_rnn.keras\") \n",
    "#print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"b-o\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"r-o\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Lowest validation MAE: \", min(val_loss) * stdSales)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29db13fd",
   "metadata": {},
   "source": [
    "# Basic GRU\n",
    "\n",
    "This is the best approach by far, just using a simple GRU without any further addition or modification, reaching the desired MAE. This might be the case due to the \"simplicity\" of the problem, where the sales patterns tend to repeat themselves (christmas sale, black friday sale, and so on) without many deviations, thus a very complex model wouldn't be efficient to solve it.\n",
    "\n",
    "Dimensionalities of 16, 32 and 48 have been tested, being 48 the one that provided the best MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe96902",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(seqLength, 6))\n",
    "\n",
    "x = layers.GRU(48)(inputs)\n",
    "\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "  \n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models2/walmart_basic_rnn.keras\", save_best_only=True, monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "#print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(trainData,\n",
    "                    epochs=50,\n",
    "                    validation_data=testData,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "#model = keras.models.load_model(\"walmart_basic_rnn.keras\") \n",
    "#print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"b-o\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"r-o\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Lowest validation MAE: \", min(val_loss) * stdSales)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67fa0391",
   "metadata": {},
   "source": [
    "# Recurrent Dropout\n",
    "\n",
    "Different dropout values have been tested, ranging from 0.1 to 0.5, but in the end this didn't help improve the MAE at all, only provided worse results. The basic GRU approach didn't show overfitting, thus makes sense that this problem does not require to apply dropout to the networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c00982",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(seqLength, 6))\n",
    "\n",
    "x = layers.LSTM(32, recurrent_dropout=0.10)(inputs)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models2/walmart_recurrent_rnn.keras\", save_best_only=True, monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "#print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(trainData,\n",
    "                    epochs=50,\n",
    "                    validation_data=testData,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6784d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"b-o\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"r-o\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Lowest validation MAE: \", min(val_loss) * stdSales)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bfc4292",
   "metadata": {},
   "source": [
    "# Stacking Recurrent Layers\n",
    "\n",
    "This approach has the potential to provide even better results than the basic GRU, but we still would have to increase the number of training epochs, thus making it extremely inefficient compared to the other approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b474223",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(seqLength, 6))\n",
    "\n",
    "x = layers.GRU(48, return_sequences=True)(inputs)\n",
    "x = layers.GRU(48)(x)\n",
    "\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "  \n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models2/walmart_stacked_rnn.keras\", save_best_only=True, monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "#print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(trainData,\n",
    "                    epochs=50,\n",
    "                    validation_data=testData,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75767a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"b-o\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"r-o\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Lowest validation MAE: \", min(val_loss) * stdSales)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd51ca1f",
   "metadata": {},
   "source": [
    "# Bidirectional RNNs\n",
    "\n",
    "The bidirectional LSTM approach also has the potential to overperform the basic GRU approach, but same as before, we would have to increase the number of epochs in order to make it viable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(seqLength, 6))\n",
    "\n",
    "x = layers.Bidirectional(layers.LSTM(32))(inputs)\n",
    "\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "  \n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models2/walmart_bidirectional_rnn.keras\", save_best_only=True, monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "#print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a944c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(trainData,\n",
    "                    epochs=50,\n",
    "                    validation_data=testData,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d07a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"b-o\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"r-o\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Lowest validation MAE: \", min(val_loss) * stdSales)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f384fd1226c4d1869a5635dd9b9dab80ee60a435542056c84979dd5de7d2d66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
